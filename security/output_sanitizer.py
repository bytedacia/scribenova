"""
Pipeline Fase 8 - Sicurezza IA: sanitizzazione output generati dal modello.
"""
import re
from typing import Optional

# Tag/script da rimuovere dall'output prima di mostrarlo all'utente
DANGEROUS_PATTERNS = [
    re.compile(r"<script[^>]*>.*?</script>", re.I | re.DOTALL),
    re.compile(r"javascript\s*:", re.I),
    re.compile(r"on\w+\s*=\s*['\"][^'\"]*['\"]", re.I),
    re.compile(r"<iframe[^>]*>", re.I),
    re.compile(r"vbscript\s*:", re.I),
]


def sanitize_model_output(text: Optional[str], max_length: int = 500000) -> str:
    """
    Sanitizza output del modello: rimuove script, event handler, tronca.
    """
    if not text:
        return ""
    s = str(text).strip()
    for pat in DANGEROUS_PATTERNS:
        s = pat.sub("", s)
    # Escape < e > residui per evitare tag malformati
    s = s.replace("<", "&lt;").replace(">", "&gt;")
    if len(s) > max_length:
        s = s[:max_length]
    return s


def add_watermark_stub(text: str, marker: str = "Fractal Nova") -> str:
    """
    Stub per watermarking: aggiunge marker invisibile o in coda (per audit).
    In produzione usare tecniche specifiche (es. pattern di spacing).
    """
    if not text or not marker:
        return text
    return text.rstrip() + f"\n\n<!-- generated by {marker} -->"
